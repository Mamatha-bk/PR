{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55e6b918-afac-43e8-b338-43f8ac64014f",
   "metadata": {},
   "source": [
    "##  Deep Neural Networks Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8e066d-2849-4a6e-9c71-bb98d605ae07",
   "metadata": {},
   "source": [
    "In this project, you will be working with a real-world data set from the Las Vegas Metropolitan Police Department. The dataset  contains information about the reported incidents, including the time and location of the crime, type of incident, and number of persons involved. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e87fac7-352a-4c39-b087-76254b5e2743",
   "metadata": {},
   "source": [
    "The dataset is downloaded from the public docket at: \n",
    "https://opendata-lvmpd.hub.arcgis.com\n",
    "\n",
    "let's read the csv file and transform the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "efeee522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\030821412\\appdata\\local\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\030821412\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\030821412\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\030821412\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\030821412\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\030821412\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\030821412\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (2023.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\030821412\\appdata\\local\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\030821412\\appdata\\local\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "637211a4-582f-426b-a127-c3f284463f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "bcf40b02-80b6-4abc-a662-f7ed50a65181",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df = pd.read_csv('datasets/LVMPD-Stats.csv', parse_dates=['ReportedOn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e1ca1d15-3955-4971-a3c4-c1a73b62edda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/LVMPD-Stats.csv', parse_dates=['ReportedOn'],\n",
    "                 usecols = ['X', 'Y', 'ReportedOn',\n",
    "                            'Area_Command','NIBRSOffenseCode',\n",
    "                            'VictimCount' ] )\n",
    "\n",
    "df['DayOfWeek'] = df['ReportedOn'].dt.day_name()\n",
    "df['Time' ]     = df['ReportedOn'].dt.hour\n",
    "df.drop(columns = 'ReportedOn', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3ddc413d-ba3f-4204-bc18-7fdd4de8d221",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['X'] = df['X'] \n",
    "df['Y'] = df['Y'] \n",
    "df['Time'] = pd.factorize(df['Time'])[0]\n",
    "df['DayOfWeek'] = pd.factorize(df['DayOfWeek'])[0]\n",
    "df.Area_Command = pd.factorize(df['Area_Command'])[0]\n",
    "df.VictimCount = pd.factorize(df['VictimCount'])[0]\n",
    "df.NIBRSOffenseCode = pd.factorize(df['NIBRSOffenseCode'])[0]\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a9c6162f-9686-4195-818d-950a6368c686",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df[['X', 'Y', 'Area_Command', 'NIBRSOffenseCode',\n",
    "       'DayOfWeek', 'Time','VictimCount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a90bc78a-6d1b-4fe4-a1b0-8333aec1c851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275, 7)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651605b1-8d2c-4d3e-a09e-9aef6e550fc6",
   "metadata": {},
   "source": [
    "# Goal\n",
    "The goal is to build a predictive model that is trained on the following data:\n",
    "* latitude and longitude (location)\n",
    "* Hour of the day\n",
    "* Day of the week\n",
    "* Area-of-command code: The police designation of the bureau of the operation.\n",
    "* Classification code for the crime committed\n",
    "  \n",
    "The predicted variable is the number of persons involved in the accident.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e54f0b8-83f9-4db9-88f9-f5a595342069",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "* print a few rows of the values in the dataframe ``df`` and explain what each column of data means. \n",
    "* identify the input and target variables\n",
    "* what is the range of values in each column? Do you need to scale, shift or normalize your data? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "779f9943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Area_Command</th>\n",
       "      <th>NIBRSOffenseCode</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Time</th>\n",
       "      <th>VictimCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-115.087518</td>\n",
       "      <td>36.216702</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-115.240172</td>\n",
       "      <td>36.189693</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-115.143088</td>\n",
       "      <td>36.181329</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-115.225014</td>\n",
       "      <td>36.117633</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-115.176708</td>\n",
       "      <td>36.095967</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            X          Y  Area_Command  NIBRSOffenseCode  DayOfWeek  Time  \\\n",
       "0 -115.087518  36.216702             0                 0          0     0   \n",
       "1 -115.240172  36.189693             1                 1          1     1   \n",
       "2 -115.143088  36.181329             2                 1          2     0   \n",
       "3 -115.225014  36.117633             3                 1          1     2   \n",
       "4 -115.176708  36.095967             4                 1          1     3   \n",
       "\n",
       "   VictimCount  \n",
       "0            0  \n",
       "1            0  \n",
       "2            1  \n",
       "3            2  \n",
       "4            0  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5566a802",
   "metadata": {},
   "source": [
    "'X' and 'Y': Latitude and longitude (location) coordinates of the crime.\n",
    "\n",
    "'Area_Command': The police designation of the bureau of operation (also encoded as a numerical value).\n",
    "\n",
    "'NIBRSOffenseCode': Classification code for the crime committed (encoded as a numerical value).\n",
    "\n",
    "'DayOfWeek': Day of the week when the crime was reported (encoded as a numerical value).\n",
    "\n",
    "'Time': Hour of the day when the crime was reported.\n",
    "\n",
    "'VictimCount': The number of persons involved in the incident (this is your target variable).\n",
    "\n",
    "\n",
    "Input Variables: 'X', 'Y', 'Time', 'DayOfWeek', 'Area_Command', 'NIBRSOffenseCode'.\n",
    "\n",
    "Target Variable: 'VictimCount'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "10806f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                X           Y  Area_Command  NIBRSOffenseCode   DayOfWeek  \\\n",
      "count  275.000000  275.000000    275.000000        275.000000  275.000000   \n",
      "mean  -115.159326   36.143360      3.978182          0.909091    2.981818   \n",
      "std      0.101294    0.118418      3.045799          0.334878    1.924590   \n",
      "min   -116.000000   35.068419      0.000000          0.000000    0.000000   \n",
      "25%   -115.209198   36.114704      1.000000          1.000000    1.000000   \n",
      "50%   -115.149945   36.152415      3.000000          1.000000    3.000000   \n",
      "75%   -115.105200   36.183854      6.000000          1.000000    5.000000   \n",
      "max   -114.625570   37.000000     11.000000          2.000000    6.000000   \n",
      "\n",
      "             Time  VictimCount  \n",
      "count  275.000000   275.000000  \n",
      "mean    11.236364     0.712727  \n",
      "std      7.039937     0.978427  \n",
      "min      0.000000     0.000000  \n",
      "25%      5.000000     0.000000  \n",
      "50%     11.000000     0.000000  \n",
      "75%     18.000000     1.000000  \n",
      "max     23.000000     6.000000  \n",
      "             X          Y  Area_Command  NIBRSOffenseCode  DayOfWeek  Time  \\\n",
      "min -116.00000  35.068419             0                 0          0     0   \n",
      "max -114.62557  37.000000            11                 2          6    23   \n",
      "\n",
      "     VictimCount  \n",
      "min            0  \n",
      "max            6  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "description = df.describe()\n",
    "\n",
    "# Display the summary statistics\n",
    "print(description)\n",
    "\n",
    "# Get the range of values for numerical columns\n",
    "numerical_columns = df.select_dtypes(include=[np.number])  # Select only numeric columns\n",
    "column_ranges = numerical_columns.agg(['min', 'max'])\n",
    "\n",
    "# Display the range of values\n",
    "print(column_ranges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3db5ef8",
   "metadata": {},
   "source": [
    "Whether you need to scale, shift, or normalize your data depends on your specific modeling approach and the requirements of the machine learning algorithm you plan to use. Many machine learning algorithms work well with standardized data (mean-centered and scaled to have unit variance), so you may choose to apply scaling or normalization if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5549ecc9-3c0b-4efa-9a1f-340a25a1e4be",
   "metadata": {},
   "source": [
    "## Task 2 \n",
    "\n",
    "* Create two `DataLoader` objects for training and testing based on the input and output variables. Pick a reasonable batch size and verify the shape of data by iterating over the one dataset and printing the shape of the batched data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "00fe4287-934b-4799-9e43-c3571acfbab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([64, 6])\n",
      "Target shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have the DataFrame 'df' from Task 1\n",
    "# Convert the DataFrame to PyTorch tensors\n",
    "X = torch.tensor(df.drop(columns=['VictimCount']).values, dtype=torch.float32)\n",
    "y = torch.tensor(df['VictimCount'].values, dtype=torch.float32)\n",
    "\n",
    "# Define a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create DataLoader objects for training and testing\n",
    "batch_size = 64  # Adjust this batch size as needed\n",
    "\n",
    "# Assuming you want an 80-20 train-test split\n",
    "split = int(0.8 * len(X))\n",
    "train_dataset = CustomDataset(X[:split], y[:split])\n",
    "test_dataset = CustomDataset(X[split:], y[split:])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Verify the shape of data by iterating over one dataset (e.g., train_loader)\n",
    "for batch in train_loader:\n",
    "    inputs, targets = batch\n",
    "    print(\"Input shape:\", inputs.shape)\n",
    "    print(\"Target shape:\", targets.shape)\n",
    "    break  # Print the shape of the first batch only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb6f08c-5e70-4b14-b62c-4686d9f7aace",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "In this task you will try to predict number of crime victims as a **real number**. Therefore the machine learning problem is a **regression** problem. \n",
    "\n",
    "* Define the proper loss function for this task\n",
    "* what should the size of the predicted output be?\n",
    "* explain your choice of architecture, including how many layers you will be using\n",
    "* define an optimizer for training this model, choose a proper learning rate \n",
    "* write a training loop that obtains a batch out of the  training data and calculates the forward and backward passes over the neural network. Call the optimizer to update the weights of the neural network.\n",
    "* write a for loop that continues the training over a number of epochs. At the end of each epoch, calculate the ``MSE`` error on the test data and print it.\n",
    "* is your model training well? Adjust the learning rate, hidden size of the network, and try different activation functions and number of layers to achieve the best accuracy and report it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e963a7f",
   "metadata": {},
   "source": [
    "1.For a regression problem where the goal is to predict a real number (in this case, the number of crime victims), the appropriate loss function is the Mean Squared Error (MSE) loss function. The MSE loss measures the average squared difference between the predicted values and the actual target values.\n",
    "\n",
    "The MSE loss is defined as:\n",
    "\n",
    "MSE = (1/N) * Σ(yᵢ - ŷᵢ)²\n",
    "\n",
    "\n",
    "2.In a regression problem, the size of the predicted output should match the dimensionality of your target variable. \n",
    "Since you're predicting the number of crime victims as a real number in this context, \n",
    " the size of the predicted output should be a single real number for each input sample in your dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ab75f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15940b9f",
   "metadata": {},
   "source": [
    "3.Proposed Architecture:\n",
    "    Input Layer: Number of neurons should match the number of features in the dataset.\n",
    "    Multiple Hidden Layers: A common approach might involve 2-3 hidden layers with varying numbers of neurons.\n",
    "    Activation Function: ReLU (Rectified Linear Unit) is commonly used in hidden layers.\n",
    "    Output Layer: A single neuron to produce the regression output (prediction for the number of crime victims). \n",
    "    \n",
    "No activation function is applied here.\n",
    "The choice of the number of layers depends on the complexity of the problem. \n",
    "For a starting point, two or three hidden layers might be sufficient, and you can adjust the number of neurons in these layers based on the complexity of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534391a7",
   "metadata": {},
   "source": [
    "4.Using Adam optimizer, a popular and versatile choice due to its adaptive learning rate and momentum capabilities, \n",
    "for training the model. The learning rate determines the step size the optimizer takes during the weight updates. \n",
    "A suitable learning rate is typically within the range of 0.001 to 0.01, \n",
    "although this can vary based on the dataset and the neural network architecture.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a4e949f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "89d4d285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Avg. Loss: 1.5442\n",
      "Epoch 1 - Test MSE: 1.5273\n",
      "Epoch 2/10, Avg. Loss: 0.9716\n",
      "Epoch 2 - Test MSE: 1.7224\n",
      "Epoch 3/10, Avg. Loss: 0.8909\n",
      "Epoch 3 - Test MSE: 1.5704\n",
      "Epoch 4/10, Avg. Loss: 0.8527\n",
      "Epoch 4 - Test MSE: 1.6878\n",
      "Epoch 5/10, Avg. Loss: 0.8203\n",
      "Epoch 5 - Test MSE: 1.5217\n",
      "Epoch 6/10, Avg. Loss: 0.8181\n",
      "Epoch 6 - Test MSE: 1.5077\n",
      "Epoch 7/10, Avg. Loss: 0.8788\n",
      "Epoch 7 - Test MSE: 1.5495\n",
      "Epoch 8/10, Avg. Loss: 0.7882\n",
      "Epoch 8 - Test MSE: 1.4930\n",
      "Epoch 9/10, Avg. Loss: 0.8539\n",
      "Epoch 9 - Test MSE: 1.5892\n",
      "Epoch 10/10, Avg. Loss: 0.7597\n",
      "Epoch 10 - Test MSE: 1.4906\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the neural network architecture\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X.shape[1]  # Input size based on the number of features\n",
    "hidden_size = 64  # Adjust the hidden layer size\n",
    "output_size = 1  # Single output for regression\n",
    "\n",
    "# Create the model, loss function, and optimizer\n",
    "model = RegressionModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust the learning rate as needed\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Number of training epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        y_pred = model(batch_X)\n",
    "        loss = criterion(y_pred, batch_y.view(-1, 1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Avg. Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0.0\n",
    "        for test_batch_X, test_batch_y in test_loader:\n",
    "            y_pred = model(test_batch_X)\n",
    "            test_loss += criterion(y_pred, test_batch_y.view(-1, 1)).item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    print(f\"Epoch {epoch + 1} - Test MSE: {avg_test_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fc4fe2",
   "metadata": {},
   "source": [
    "Evaluate the model's performance using the test data. If the model is not training well, you can adjust hyperparameters such as the learning rate, hidden layer size, activation functions, and the number of layers to achieve better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2b269787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Avg. Loss: 6.2065\n",
      "Epoch 1 - Test MSE: 4.7824\n",
      "Epoch 2/10, Avg. Loss: 2.6870\n",
      "Epoch 2 - Test MSE: 2.3425\n",
      "Epoch 3/10, Avg. Loss: 1.0924\n",
      "Epoch 3 - Test MSE: 1.5794\n",
      "Epoch 4/10, Avg. Loss: 0.9933\n",
      "Epoch 4 - Test MSE: 1.6765\n",
      "Epoch 5/10, Avg. Loss: 1.2639\n",
      "Epoch 5 - Test MSE: 1.8051\n",
      "Epoch 6/10, Avg. Loss: 1.3214\n",
      "Epoch 6 - Test MSE: 1.7143\n",
      "Epoch 7/10, Avg. Loss: 1.1186\n",
      "Epoch 7 - Test MSE: 1.5822\n",
      "Epoch 8/10, Avg. Loss: 0.9483\n",
      "Epoch 8 - Test MSE: 1.5550\n",
      "Epoch 9/10, Avg. Loss: 0.9651\n",
      "Epoch 9 - Test MSE: 1.6294\n",
      "Epoch 10/10, Avg. Loss: 0.8416\n",
      "Epoch 10 - Test MSE: 1.6778\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the neural network architecture\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X.shape[1]  # Input size based on the number of features\n",
    "hidden_size = 64  # Adjust the hidden layer size\n",
    "output_size = 1  # Single output for regression\n",
    "\n",
    "# Create the model, loss function, and optimizer\n",
    "model = RegressionModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Adjust the learning rate as needed\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Number of training epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        y_pred = model(batch_X)\n",
    "        loss = criterion(y_pred, batch_y.view(-1, 1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Avg. Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0.0\n",
    "        for test_batch_X, test_batch_y in test_loader:\n",
    "            y_pred = model(test_batch_X)\n",
    "            test_loss += criterion(y_pred, test_batch_y.view(-1, 1)).item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    print(f\"Epoch {epoch + 1} - Test MSE: {avg_test_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e3fc70-c6ce-4589-9930-128951290e8d",
   "metadata": {},
   "source": [
    "## Task 4 \n",
    "\n",
    "In this task, you will try to predict the number of crime victims as a **class number**. Therefore the machine learning problem is a **classification** problem. \n",
    "\n",
    "* Repeat all the steps in task 3. Specifically, pay attention to the differences with regression.\n",
    "* How would you find the number of classes on the output data?\n",
    "* How is the architecture different?\n",
    "* How is the loss function different?\n",
    "* Calculate the Accuracy for test data as the number of correct classified outputs divided by the total number of test data in each epoch. Report it at the end of each epoch\n",
    "* Try a few variations of learning rate, hidden dimensions, layers, etc. What is the best accuracy that you can get? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d5dd9656",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = df['VictimCount'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e1666c",
   "metadata": {},
   "source": [
    "Differences\n",
    "Output Layer: For regression tasks, the output layer typically consists of a single neuron with linear or no activation. In contrast, for classification tasks, the number of output neurons matches the number of classes with specific activation functions.\n",
    "Loss Function: Different loss functions are used based on the task type - MSE, MAE for regression, and Cross-Entropy based losses for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c3bcd96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 49.09%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ca2df11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping. Best test accuracy: 60.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Load your dataset or create sample data\n",
    "# Example: df = pd.read_csv('your_data.csv')\n",
    "X = df.drop(columns='VictimCount').values  # Extract feature values\n",
    "y = df['VictimCount'].values  # Extract class labels\n",
    "\n",
    "# Convert the data to appropriate data types\n",
    "X = torch.tensor(X, dtype=torch.float32)  # Assuming features are numeric\n",
    "y = torch.tensor(y, dtype=torch.int64)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data preprocessing\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)  # Standardize the training data\n",
    "X_test = scaler.transform(X_test)  # Standardize the testing data\n",
    "\n",
    "# Convert class labels to LongTensor\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "# Define your neural network model and hyperparameters\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "unique_classes = np.unique(y)\n",
    "num_classes = len(unique_classes)\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 32  # Increase hidden units\n",
    "output_size = num_classes\n",
    "\n",
    "model = ClassificationModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.1\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create DataLoader objects for training and testing\n",
    "batch_size = 64 # Adjust as needed\n",
    "train_dataset = TensorDataset(torch.Tensor(X_train), y_train)\n",
    "test_dataset = TensorDataset(torch.Tensor(X_test), y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "# Early stopping\n",
    "best_accuracy = 0\n",
    "early_stopping_patience = 10\n",
    "no_improvement = 0\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        y_pred = model(batch_X)\n",
    "        loss = criterion(y_pred, batch_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Evaluation on the test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_correct = 0\n",
    "\n",
    "        for test_batch_X, test_batch_y in test_loader:\n",
    "            y_pred = model(test_batch_X)\n",
    "            total_correct += (torch.argmax(y_pred, dim=1) == test_batch_y).sum().item()\n",
    "\n",
    "        test_accuracy = total_correct / len(test_loader.dataset)\n",
    "\n",
    "     \n",
    "        # Learning rate scheduler step based on test accuracy\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "            no_improvement = 0\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "            if no_improvement >= early_stopping_patience:\n",
    "                print(\"Early stopping. Best test accuracy:\", best_accuracy*100)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ddd73d",
   "metadata": {},
   "source": [
    "Best accuracy we got 60 at hidden_size = 32 and learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e05e2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84d2a304-6197-4cd9-b31e-745f7862f213",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e4ef16-d828-45e5-bd58-1c49fcfecf52",
   "metadata": {},
   "source": [
    "### Reflect on your results\n",
    "\n",
    "* Write a paragraph about your experience with tasks 3 and 4. How do you compare the results? Which one worked better? Why?\n",
    "* Write a piece of code that finds an example of a  miss-classification. Calculate the probabilities for the output classes and plot them in a bar chart. Also, indicate what is the correct class label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b000b0b-fa37-4f8d-8ec7-0a1e6e749424",
   "metadata": {},
   "source": [
    "## Task 6: Exploring the patterns in raw data\n",
    "\n",
    "* Plot the crime incidents as a `scatter` plot using the corrdinates. Use the color property of each datapoint to indicate the day of the week. Is there a pattern in the plot?\n",
    "* Now make a new scatter plot and use the color property of each datapoint to indicate the number of persons involved in the incident. Is there a pattern here?\n",
    "* use numpy (or pandas if you like) to sort the number of crimes reported by the day of the week. What days are most frequent?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b396aaf-5d1a-49cd-ae1d-b63af7e561f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
