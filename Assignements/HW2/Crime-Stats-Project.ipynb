{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55e6b918-afac-43e8-b338-43f8ac64014f",
   "metadata": {},
   "source": [
    "##  Deep Neural Networks Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8e066d-2849-4a6e-9c71-bb98d605ae07",
   "metadata": {},
   "source": [
    "In this project, you will be working with a real-world data set from the Las Vegas Metropolitan Police Department. The dataset  contains information about the reported incidents, including the time and location of the crime, type of incident, and number of persons involved. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e87fac7-352a-4c39-b087-76254b5e2743",
   "metadata": {},
   "source": [
    "The dataset is downloaded from the public docket at: \n",
    "https://opendata-lvmpd.hub.arcgis.com\n",
    "\n",
    "let's read the csv file and transform the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efeee522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\030821412\\appdata\\local\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\030821412\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\030821412\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\030821412\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\030821412\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\030821412\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\030821412\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (2023.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\030821412\\appdata\\local\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\030821412\\appdata\\local\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "637211a4-582f-426b-a127-c3f284463f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcf40b02-80b6-4abc-a662-f7ed50a65181",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df = pd.read_csv('datasets/LVMPD-Stats.csv', parse_dates=['ReportedOn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1ca1d15-3955-4971-a3c4-c1a73b62edda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/LVMPD-Stats.csv', parse_dates=['ReportedOn'],\n",
    "                 usecols = ['X', 'Y', 'ReportedOn',\n",
    "                            'Area_Command','NIBRSOffenseCode',\n",
    "                            'VictimCount' ] )\n",
    "\n",
    "df['DayOfWeek'] = df['ReportedOn'].dt.day_name()\n",
    "df['Time' ]     = df['ReportedOn'].dt.hour\n",
    "df.drop(columns = 'ReportedOn', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ddc413d-ba3f-4204-bc18-7fdd4de8d221",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['X'] = df['X'] \n",
    "df['Y'] = df['Y'] \n",
    "df['Time'] = pd.factorize(df['Time'])[0]\n",
    "df['DayOfWeek'] = pd.factorize(df['DayOfWeek'])[0]\n",
    "df.Area_Command = pd.factorize(df['Area_Command'])[0]\n",
    "df.VictimCount = pd.factorize(df['VictimCount'])[0]\n",
    "df.NIBRSOffenseCode = pd.factorize(df['NIBRSOffenseCode'])[0]\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9c6162f-9686-4195-818d-950a6368c686",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df[['X', 'Y', 'Area_Command', 'NIBRSOffenseCode',\n",
    "       'DayOfWeek', 'Time','VictimCount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a90bc78a-6d1b-4fe4-a1b0-8333aec1c851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651605b1-8d2c-4d3e-a09e-9aef6e550fc6",
   "metadata": {},
   "source": [
    "# Goal\n",
    "The goal is to build a predictive model that is trained on the following data:\n",
    "* latitude and longitude (location)\n",
    "* Hour of the day\n",
    "* Day of the week\n",
    "* Area-of-command code: The police designation of the bureau of the operation.\n",
    "* Classification code for the crime committed\n",
    "  \n",
    "The predicted variable is the number of persons involved in the accident.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e54f0b8-83f9-4db9-88f9-f5a595342069",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "* print a few rows of the values in the dataframe ``df`` and explain what each column of data means. \n",
    "* identify the input and target variables\n",
    "* what is the range of values in each column? Do you need to scale, shift or normalize your data? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8555e0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Area_Command</th>\n",
       "      <th>NIBRSOffenseCode</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Time</th>\n",
       "      <th>VictimCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-115.087518</td>\n",
       "      <td>36.216702</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-115.240172</td>\n",
       "      <td>36.189693</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-115.143088</td>\n",
       "      <td>36.181329</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-115.225014</td>\n",
       "      <td>36.117633</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-115.176708</td>\n",
       "      <td>36.095967</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            X          Y  Area_Command  NIBRSOffenseCode  DayOfWeek  Time  \\\n",
       "0 -115.087518  36.216702             0                 0          0     0   \n",
       "1 -115.240172  36.189693             1                 1          1     1   \n",
       "2 -115.143088  36.181329             2                 1          2     0   \n",
       "3 -115.225014  36.117633             3                 1          1     2   \n",
       "4 -115.176708  36.095967             4                 1          1     3   \n",
       "\n",
       "   VictimCount  \n",
       "0            0  \n",
       "1            0  \n",
       "2            1  \n",
       "3            2  \n",
       "4            0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc1088b",
   "metadata": {},
   "source": [
    "'X' and 'Y': Latitude and longitude (location) coordinates of the crime.\n",
    "\n",
    "'Area_Command': The police designation of the bureau of operation (also encoded as a numerical value).\n",
    "\n",
    "'NIBRSOffenseCode': Classification code for the crime committed (encoded as a numerical value).\n",
    "\n",
    "'DayOfWeek': Day of the week when the crime was reported (encoded as a numerical value).\n",
    "\n",
    "'Time': Hour of the day when the crime was reported.\n",
    "\n",
    "'VictimCount': The number of persons involved in the incident (this is your target variable).\n",
    "\n",
    "\n",
    "Input Variables: 'X', 'Y', 'Time', 'DayOfWeek', 'Area_Command', 'NIBRSOffenseCode'.\n",
    "\n",
    "Target Variable: 'VictimCount'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afd12db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                X           Y  Area_Command  NIBRSOffenseCode   DayOfWeek  \\\n",
      "count  275.000000  275.000000    275.000000        275.000000  275.000000   \n",
      "mean  -115.159326   36.143360      3.978182          0.909091    2.981818   \n",
      "std      0.101294    0.118418      3.045799          0.334878    1.924590   \n",
      "min   -116.000000   35.068419      0.000000          0.000000    0.000000   \n",
      "25%   -115.209198   36.114704      1.000000          1.000000    1.000000   \n",
      "50%   -115.149945   36.152415      3.000000          1.000000    3.000000   \n",
      "75%   -115.105200   36.183854      6.000000          1.000000    5.000000   \n",
      "max   -114.625570   37.000000     11.000000          2.000000    6.000000   \n",
      "\n",
      "             Time  VictimCount  \n",
      "count  275.000000   275.000000  \n",
      "mean    11.236364     0.712727  \n",
      "std      7.039937     0.978427  \n",
      "min      0.000000     0.000000  \n",
      "25%      5.000000     0.000000  \n",
      "50%     11.000000     0.000000  \n",
      "75%     18.000000     1.000000  \n",
      "max     23.000000     6.000000  \n",
      "             X          Y  Area_Command  NIBRSOffenseCode  DayOfWeek  Time  \\\n",
      "min -116.00000  35.068419             0                 0          0     0   \n",
      "max -114.62557  37.000000            11                 2          6    23   \n",
      "\n",
      "     VictimCount  \n",
      "min            0  \n",
      "max            6  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "description = df.describe()\n",
    "\n",
    "# Display the summary statistics\n",
    "print(description)\n",
    "\n",
    "# Get the range of values for numerical columns\n",
    "numerical_columns = df.select_dtypes(include=[np.number])  # Select only numeric columns\n",
    "column_ranges = numerical_columns.agg(['min', 'max'])\n",
    "\n",
    "# Display the range of values\n",
    "print(column_ranges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7149af5",
   "metadata": {},
   "source": [
    "Whether you need to scale, shift, or normalize your data depends on your specific modeling approach and the requirements of the machine learning algorithm you plan to use. Many machine learning algorithms work well with standardized data (mean-centered and scaled to have unit variance), so you may choose to apply scaling or normalization if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5549ecc9-3c0b-4efa-9a1f-340a25a1e4be",
   "metadata": {},
   "source": [
    "## Task 2 \n",
    "\n",
    "* Create two `DataLoader` objects for training and testing based on the input and output variables. Pick a reasonable batch size and verify the shape of data by iterating over the one dataset and printing the shape of the batched data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00fe4287-934b-4799-9e43-c3571acfbab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([64, 6])\n",
      "Target shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have the DataFrame 'df' from Task 1\n",
    "# Convert the DataFrame to PyTorch tensors\n",
    "X = torch.tensor(df.drop(columns=['VictimCount']).values, dtype=torch.float32)\n",
    "y = torch.tensor(df['VictimCount'].values, dtype=torch.float32)\n",
    "\n",
    "# Define a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create DataLoader objects for training and testing\n",
    "batch_size = 64  # Adjust this batch size as needed\n",
    "\n",
    "# Assuming you want an 80-20 train-test split\n",
    "split = int(0.8 * len(X))\n",
    "train_dataset = CustomDataset(X[:split], y[:split])\n",
    "test_dataset = CustomDataset(X[split:], y[split:])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Verify the shape of data by iterating over one dataset (e.g., train_loader)\n",
    "for batch in train_loader:\n",
    "    inputs, targets = batch\n",
    "    print(\"Input shape:\", inputs.shape)\n",
    "    print(\"Target shape:\", targets.shape)\n",
    "    break  # Print the shape of the first batch only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb6f08c-5e70-4b14-b62c-4686d9f7aace",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "In this task you will try to predict number of crime victims as a **real number**. Therefore the machine learning problem is a **regression** problem. \n",
    "\n",
    "* Define the proper loss function for this task\n",
    "* what should the size of the predicted output be?\n",
    "* explain your choice of architecture, including how many layers you will be using\n",
    "* define an optimizer for training this model, choose a proper learning rate \n",
    "* write a training loop that obtains a batch out of the  training data and calculates the forward and backward passes over the neural network. Call the optimizer to update the weights of the neural network.\n",
    "* write a for loop that continues the training over a number of epochs. At the end of each epoch, calculate the ``MSE`` error on the test data and print it.\n",
    "* is your model training well? Adjust the learning rate, hidden size of the network, and try different activation functions and number of layers to achieve the best accuracy and report it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b881248",
   "metadata": {},
   "source": [
    "1.For a regression problem where the goal is to predict a real number (in this case, the number of crime victims), the appropriate loss function is the Mean Squared Error (MSE) loss function. The MSE loss measures the average squared difference between the predicted values and the actual target values.\n",
    "\n",
    "The MSE loss is defined as:\n",
    "\n",
    "MSE = (1/N) * Σ(yᵢ - ŷᵢ)²\n",
    "\n",
    "\n",
    "2.In a regression problem, the size of the predicted output should match the dimensionality of your target variable. \n",
    "Since you're predicting the number of crime victims as a real number in this context, \n",
    " the size of the predicted output should be a single real number for each input sample in your dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e22f0ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4366704",
   "metadata": {},
   "source": [
    "3.Proposed Architecture:\n",
    "    Input Layer: Number of neurons should match the number of features in the dataset.\n",
    "    Multiple Hidden Layers: A common approach might involve 2-3 hidden layers with varying numbers of neurons.\n",
    "    Activation Function: ReLU (Rectified Linear Unit) is commonly used in hidden layers.\n",
    "    Output Layer: A single neuron to produce the regression output (prediction for the number of crime victims). \n",
    "    \n",
    "No activation function is applied here.\n",
    "The choice of the number of layers depends on the complexity of the problem. \n",
    "For a starting point, two or three hidden layers might be sufficient, and you can adjust the number of neurons in these layers based on the complexity of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05f266d",
   "metadata": {},
   "source": [
    "4.Using Adam optimizer, a popular and versatile choice due to its adaptive learning rate and momentum capabilities, \n",
    "for training the model. The learning rate determines the step size the optimizer takes during the weight updates. \n",
    "A suitable learning rate is typically within the range of 0.001 to 0.01, \n",
    "although this can vary based on the dataset and the neural network architecture.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6112a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97d52965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Avg. Loss: 5.7567\n",
      "Epoch 1 - Test MSE: 3.9843\n",
      "Epoch 2/10, Avg. Loss: 4.4217\n",
      "Epoch 2 - Test MSE: 2.4683\n",
      "Epoch 3/10, Avg. Loss: 1.1643\n",
      "Epoch 3 - Test MSE: 2.9868\n",
      "Epoch 4/10, Avg. Loss: 2.2454\n",
      "Epoch 4 - Test MSE: 2.7468\n",
      "Epoch 5/10, Avg. Loss: 1.1906\n",
      "Epoch 5 - Test MSE: 1.6184\n",
      "Epoch 6/10, Avg. Loss: 1.2805\n",
      "Epoch 6 - Test MSE: 1.8460\n",
      "Epoch 7/10, Avg. Loss: 1.0855\n",
      "Epoch 7 - Test MSE: 1.6085\n",
      "Epoch 8/10, Avg. Loss: 0.8923\n",
      "Epoch 8 - Test MSE: 1.8954\n",
      "Epoch 9/10, Avg. Loss: 0.9642\n",
      "Epoch 9 - Test MSE: 1.5442\n",
      "Epoch 10/10, Avg. Loss: 0.8515\n",
      "Epoch 10 - Test MSE: 1.5667\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the neural network architecture\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X.shape[1]  # Input size based on the number of features\n",
    "hidden_size = 64  # Adjust the hidden layer size\n",
    "output_size = 1  # Single output for regression\n",
    "\n",
    "# Create the model, loss function, and optimizer\n",
    "model = RegressionModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust the learning rate as needed\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Number of training epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        y_pred = model(batch_X)\n",
    "        loss = criterion(y_pred, batch_y.view(-1, 1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Avg. Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0.0\n",
    "        for test_batch_X, test_batch_y in test_loader:\n",
    "            y_pred = model(test_batch_X)\n",
    "            test_loss += criterion(y_pred, test_batch_y.view(-1, 1)).item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    print(f\"Epoch {epoch + 1} - Test MSE: {avg_test_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eaf1ae",
   "metadata": {},
   "source": [
    "Evaluate the model's performance using the test data. If the model is not training well, you can adjust hyperparameters such as the learning rate, hidden layer size, activation functions, and the number of layers to achieve better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "631a58cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Avg. Loss: 56.1821\n",
      "Epoch 1 - Test MSE: 45.1370\n",
      "Epoch 2/10, Avg. Loss: 41.8117\n",
      "Epoch 2 - Test MSE: 32.5611\n",
      "Epoch 3/10, Avg. Loss: 29.8366\n",
      "Epoch 3 - Test MSE: 22.4102\n",
      "Epoch 4/10, Avg. Loss: 20.0311\n",
      "Epoch 4 - Test MSE: 14.6140\n",
      "Epoch 5/10, Avg. Loss: 12.8458\n",
      "Epoch 5 - Test MSE: 8.9812\n",
      "Epoch 6/10, Avg. Loss: 7.5109\n",
      "Epoch 6 - Test MSE: 5.2248\n",
      "Epoch 7/10, Avg. Loss: 4.0817\n",
      "Epoch 7 - Test MSE: 2.9855\n",
      "Epoch 8/10, Avg. Loss: 2.2198\n",
      "Epoch 8 - Test MSE: 1.8914\n",
      "Epoch 9/10, Avg. Loss: 1.2203\n",
      "Epoch 9 - Test MSE: 1.5285\n",
      "Epoch 10/10, Avg. Loss: 0.8870\n",
      "Epoch 10 - Test MSE: 1.5536\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the neural network architecture\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X.shape[1]  # Input size based on the number of features\n",
    "hidden_size = 64  # Adjust the hidden layer size\n",
    "output_size = 1  # Single output for regression\n",
    "\n",
    "# Create the model, loss function, and optimizer\n",
    "model = RegressionModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Adjust the learning rate as needed\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Number of training epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        y_pred = model(batch_X)\n",
    "        loss = criterion(y_pred, batch_y.view(-1, 1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Avg. Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0.0\n",
    "        for test_batch_X, test_batch_y in test_loader:\n",
    "            y_pred = model(test_batch_X)\n",
    "            test_loss += criterion(y_pred, test_batch_y.view(-1, 1)).item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    print(f\"Epoch {epoch + 1} - Test MSE: {avg_test_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e3fc70-c6ce-4589-9930-128951290e8d",
   "metadata": {},
   "source": [
    "## Task 4 \n",
    "\n",
    "In this task, you will try to predict the number of crime victims as a **class number**. Therefore the machine learning problem is a **classification** problem. \n",
    "\n",
    "* Repeat all the steps in task 3. Specifically, pay attention to the differences with regression.\n",
    "* How would you find the number of classes on the output data?\n",
    "* How is the architecture different?\n",
    "* How is the loss function different?\n",
    "* Calculate the Accuracy for test data as the number of correct classified outputs divided by the total number of test data in each epoch. Report it at the end of each epoch\n",
    "* Try a few variations of learning rate, hidden dimensions, layers, etc. What is the best accuracy that you can get? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "135018a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = df['VictimCount'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b6aa0a",
   "metadata": {},
   "source": [
    "Differences\n",
    "Output Layer: For regression tasks, the output layer typically consists of a single neuron with linear or no activation. In contrast, for classification tasks, the number of output neurons matches the number of classes with specific activation functions.\n",
    "Loss Function: Different loss functions are used based on the task type - MSE, MAE for regression, and Cross-Entropy based losses for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77c59e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Avg. Loss: 2.6118, Test Accuracy: 45.4545\n",
      "Epoch 2/50, Avg. Loss: 1.5265, Test Accuracy: 43.6364\n",
      "Epoch 3/50, Avg. Loss: 1.5944, Test Accuracy: 52.7273\n",
      "Epoch 4/50, Avg. Loss: 1.4419, Test Accuracy: 30.9091\n",
      "Epoch 5/50, Avg. Loss: 1.3218, Test Accuracy: 32.7273\n",
      "Epoch 6/50, Avg. Loss: 1.2357, Test Accuracy: 58.1818\n",
      "Epoch 7/50, Avg. Loss: 1.2216, Test Accuracy: 60.0000\n",
      "Epoch 8/50, Avg. Loss: 1.1732, Test Accuracy: 56.3636\n",
      "Epoch 9/50, Avg. Loss: 1.1307, Test Accuracy: 50.9091\n",
      "Epoch 10/50, Avg. Loss: 1.1935, Test Accuracy: 52.7273\n",
      "Epoch 11/50, Avg. Loss: 1.1894, Test Accuracy: 43.6364\n",
      "Epoch 12/50, Avg. Loss: 1.0630, Test Accuracy: 50.9091\n",
      "Epoch 13/50, Avg. Loss: 1.1301, Test Accuracy: 49.0909\n",
      "Epoch 14/50, Avg. Loss: 1.1498, Test Accuracy: 52.7273\n",
      "Epoch 15/50, Avg. Loss: 1.1579, Test Accuracy: 58.1818\n",
      "Epoch 16/50, Avg. Loss: 1.1715, Test Accuracy: 45.4545\n",
      "Epoch 17/50, Avg. Loss: 1.0745, Test Accuracy: 52.7273\n",
      "Early stopping. Best test accuracy: 60.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Load your dataset or create sample data\n",
    "# Example: df = pd.read_csv('your_data.csv')\n",
    "X = df.drop(columns='VictimCount').values  # Extract feature values\n",
    "y = df['VictimCount'].values  # Extract class labels\n",
    "\n",
    "# Convert the data to appropriate data types\n",
    "X = torch.tensor(X, dtype=torch.float32)  # Assuming features are numeric\n",
    "y = torch.tensor(y, dtype=torch.int64)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data preprocessing\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)  # Standardize the training data\n",
    "X_test = scaler.transform(X_test)  # Standardize the testing data\n",
    "\n",
    "# Convert class labels to LongTensor\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "# Define your neural network model and hyperparameters\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "unique_classes = np.unique(y)\n",
    "num_classes = len(unique_classes)\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 32  # Increase hidden units\n",
    "output_size = num_classes\n",
    "\n",
    "model = ClassificationModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.1\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create DataLoader objects for training and testing\n",
    "batch_size = 64 # Adjust as needed\n",
    "train_dataset = TensorDataset(torch.Tensor(X_train), y_train)\n",
    "test_dataset = TensorDataset(torch.Tensor(X_test), y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "# Early stopping\n",
    "best_accuracy = 0\n",
    "early_stopping_patience = 10\n",
    "no_improvement = 0\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        y_pred = model(batch_X)\n",
    "        loss = criterion(y_pred, batch_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Evaluation on the test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_correct = 0\n",
    "\n",
    "        for test_batch_X, test_batch_y in test_loader:\n",
    "            y_pred = model(test_batch_X)\n",
    "            total_correct += (torch.argmax(y_pred, dim=1) == test_batch_y).sum().item()\n",
    "\n",
    "        test_accuracy = total_correct / len(test_loader.dataset)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Avg. Loss: {avg_loss:.4f}, Test Accuracy: {test_accuracy*100:.4f}\")\n",
    "\n",
    "        # Learning rate scheduler step based on test accuracy\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "            no_improvement = 0\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "            if no_improvement >= early_stopping_patience:\n",
    "                print(\"Early stopping. Best test accuracy:\", best_accuracy*100)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee813d12",
   "metadata": {},
   "source": [
    "Best accuracy we got 60 at hidden_size = 32 and learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16542f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84d2a304-6197-4cd9-b31e-745f7862f213",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e4ef16-d828-45e5-bd58-1c49fcfecf52",
   "metadata": {},
   "source": [
    "### Reflect on your results\n",
    "\n",
    "* Write a paragraph about your experience with tasks 3 and 4. How do you compare the results? Which one worked better? Why?\n",
    "* Write a piece of code that finds an example of a  miss-classification. Calculate the probabilities for the output classes and plot them in a bar chart. Also, indicate what is the correct class label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ca68a8",
   "metadata": {},
   "source": [
    "Comparing the results, it is challenging to definitively say which one worked better, as it largely depends on the specific problem at hand. Regression models are more suitable when you need to predict continuous values with potentially significant variability. On the other hand, classification is appropriate when dealing with distinct categories, even if the output categories may not be entirely intuitive.\n",
    "\n",
    "The choice between regression and classification depends on the problem's nature, the available data, and the desired outcome. Ultimately, the \"better\" approach is the one that fulfills the specific goals and requirements of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1bd1d44c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 0 with shape (7,) and arg 1 with shape (32,).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m class_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(class_probabilities, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     17\u001b[0m correct_class_label \u001b[38;5;241m=\u001b[39m example_y\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 19\u001b[0m plt\u001b[38;5;241m.\u001b[39mbar([\u001b[38;5;28mstr\u001b[39m(label) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m class_labels], class_probs)\n\u001b[0;32m     20\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProbability\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\matplotlib\\pyplot.py:2439\u001b[0m, in \u001b[0;36mbar\u001b[1;34m(x, height, width, bottom, align, data, **kwargs)\u001b[0m\n\u001b[0;32m   2435\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mbar)\n\u001b[0;32m   2436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbar\u001b[39m(\n\u001b[0;32m   2437\u001b[0m         x, height, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, bottom\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, align\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcenter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   2438\u001b[0m         data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 2439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gca()\u001b[38;5;241m.\u001b[39mbar(\n\u001b[0;32m   2440\u001b[0m         x, height, width\u001b[38;5;241m=\u001b[39mwidth, bottom\u001b[38;5;241m=\u001b[39mbottom, align\u001b[38;5;241m=\u001b[39malign,\n\u001b[0;32m   2441\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\matplotlib\\__init__.py:1442\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1439\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(ax, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(sanitize_sequence, args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1444\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1445\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1446\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:2417\u001b[0m, in \u001b[0;36mAxes.bar\u001b[1;34m(self, x, height, width, bottom, align, **kwargs)\u001b[0m\n\u001b[0;32m   2414\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m yerr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2415\u001b[0m         yerr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_dx(yerr, y0, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_yunits)\n\u001b[1;32m-> 2417\u001b[0m x, height, width, y, linewidth, hatch \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbroadcast_arrays(\n\u001b[0;32m   2418\u001b[0m     \u001b[38;5;66;03m# Make args iterable too.\u001b[39;00m\n\u001b[0;32m   2419\u001b[0m     np\u001b[38;5;241m.\u001b[39matleast_1d(x), height, width, y, linewidth, hatch)\n\u001b[0;32m   2421\u001b[0m \u001b[38;5;66;03m# Now that units have been converted, set the tick locations.\u001b[39;00m\n\u001b[0;32m   2422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orientation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mbroadcast_arrays\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\numpy\\lib\\stride_tricks.py:540\u001b[0m, in \u001b[0;36mbroadcast_arrays\u001b[1;34m(subok, *args)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;66;03m# nditer is not used here to avoid the limit of 32 arrays.\u001b[39;00m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Otherwise, something like the following one-liner would suffice:\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;66;03m# return np.nditer(args, flags=['multi_index', 'zerosize_ok'],\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;66;03m#                  order='C').itviews\u001b[39;00m\n\u001b[0;32m    538\u001b[0m args \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray(_m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, subok\u001b[38;5;241m=\u001b[39msubok) \u001b[38;5;28;01mfor\u001b[39;00m _m \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m--> 540\u001b[0m shape \u001b[38;5;241m=\u001b[39m _broadcast_shape(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(array\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m shape \u001b[38;5;28;01mfor\u001b[39;00m array \u001b[38;5;129;01min\u001b[39;00m args):\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;66;03m# Common case where nothing needs to be broadcasted.\u001b[39;00m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\numpy\\lib\\stride_tricks.py:422\u001b[0m, in \u001b[0;36m_broadcast_shape\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the shape of the arrays that would result from broadcasting the\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03msupplied arrays against each other.\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# use the old-iterator because np.nditer does not handle size 0 arrays\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m# consistently\u001b[39;00m\n\u001b[1;32m--> 422\u001b[0m b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbroadcast(\u001b[38;5;241m*\u001b[39margs[:\u001b[38;5;241m32\u001b[39m])\n\u001b[0;32m    423\u001b[0m \u001b[38;5;66;03m# unfortunately, it cannot handle 32 or more arguments directly\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;28mlen\u001b[39m(args), \u001b[38;5;241m31\u001b[39m):\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;66;03m# ironically, np.broadcast does not properly handle np.broadcast\u001b[39;00m\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;66;03m# objects (it treats them as scalars)\u001b[39;00m\n\u001b[0;32m    427\u001b[0m     \u001b[38;5;66;03m# use broadcasting to avoid allocating the full array\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 0 with shape (7,) and arg 1 with shape (32,)."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "example_index = 0\n",
    "\n",
    "example_X, example_y = test_dataset[example_index]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    example_X = example_X.unsqueeze(0)\n",
    "    class_probabilities = model(example_X)\n",
    "    predicted_class = torch.argmax(class_probabilities, dim=1).item()\n",
    "\n",
    "class_labels = range(num_classes)\n",
    "class_probs = torch.softmax(class_probabilities, dim=1).squeeze().numpy()\n",
    "\n",
    "correct_class_label = example_y.item()\n",
    "\n",
    "plt.bar([str(label) for label in class_labels], class_probs)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Class Probabilities for Example')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Correct Class Label: {correct_class_label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b000b0b-fa37-4f8d-8ec7-0a1e6e749424",
   "metadata": {},
   "source": [
    "## Task 6: Exploring the patterns in raw data\n",
    "\n",
    "* Plot the crime incidents as a `scatter` plot using the corrdinates. Use the color property of each datapoint to indicate the day of the week. Is there a pattern in the plot?\n",
    "* Now make a new scatter plot and use the color property of each datapoint to indicate the number of persons involved in the incident. Is there a pattern here?\n",
    "* use numpy (or pandas if you like) to sort the number of crimes reported by the day of the week. What days are most frequent?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b396aaf-5d1a-49cd-ae1d-b63af7e561f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
